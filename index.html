<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="true" />










  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.5.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Feng Wang, happynear, deep learning" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.5.0" />






<meta name="description" content="I am a Ph. D. student at UESTC. My research intrest includes Deep Learning and Facial Image Analysis.">
<meta property="og:type" content="website">
<meta property="og:title" content="happynear's blog">
<meta property="og:url" content="http://happynear.wang/index.html">
<meta property="og:site_name" content="happynear's blog">
<meta property="og:description" content="I am a Ph. D. student at UESTC. My research intrest includes Deep Learning and Facial Image Analysis.">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="happynear's blog">
<meta name="twitter:description" content="I am a Ph. D. student at UESTC. My research intrest includes Deep Learning and Facial Image Analysis.">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"always"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: undefined,
      author: 'Author'
    }
  };
</script>

  <title> happynear's blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">happynear's blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/04/19/Normalization-Propagation-Stride/" itemprop="url">
                  Normalizing All Layers： Stride
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-04-19T01:53:53+08:00" content="2016-04-19">
              2016-04-19
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/04/19/Normalization-Propagation-Stride/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/04/19/Normalization-Propagation-Stride/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/04/19/Normalization-Propagation-Stride/" class="leancloud_visitors" data-flag-title="Normalizing All Layers： Stride">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>In the <a href="http://happynear.wang/2016/03/28/Normalizing-All-Layers%EF%BC%9A-Back-Propagation/">last post</a>, we have discussed how to normalize the gradients in the back propagate procedure. However, we leftover a problem about the stride parameter of the convolution layer and pooling layer. It is not a easy task so I tend to open a new post to discuss it.</p>
<p>In this article, we are looking at a convolution layer or pooling layer with <span class="math inline">\(w\times w\)</span> window and <span class="math inline">\(s\times s\)</span> stride. These two symbols are all the same in the following paragraphs. We will use FP to infer to the forward propagation and BP for backward propagation in short.</p>
<p>In the FP, we do not need to consider the stride parameter because every output pixel accumulates values from all input pixels of <span class="math inline">\(w\times w\)</span>, no matter how much pixel strides are applied. However, in the BP procedure, each output pixel (input in FP) correspond to only a small subset of input pixels. Different from striding on the feature map during FP, we do stride on the kernel in BP. I have drawn a picture to illustrate this procedure.</p>
<div class="figure">
<img src="http://happynear.wang/images/stride/stride.png" alt="Figure 1. Illustration of the role of the stride parameter.">
<p class="caption">Figure 1. Illustration of the role of the stride parameter.</p>
</div>
<p>As shown above, a input feature map is convolved by a <span class="math inline">\(3\times 3\)</span> filter with <span class="math inline">\(2\times 2\)</span> stride. We can see that different values on input map are participate in the convolution with different times. <code>7</code>, <code>9</code>, <code>17</code>, <code>19</code> are convolved only once, while <code>13</code> convolved <span class="math inline">\(4\)</span> times. Since BP is actually the inverse procedure of FP in convolution layer, if the kernel is all flat, the graident at position <code>7</code> will be <span class="math inline">\(4\)</span> times smaller compared with the gradient at position <code>13</code>.</p>
<p>There are two ways to normalize the output gradient. The first one is to scale the entire output gradient map. Please note that the <code>Multiplication Count</code> is constituted by some repeated cells, e.g. [4 2; 2 1] in the above figure. Then we can calculate the std of the output gradient:</p>
<p><span class="math display">\[Std[dx] = \sqrt{\frac{1}{4}(4^2 + 2^2+2^2+1^2)} = \frac{5}{2}.\]</span></p>
<p>Don’t forget that we have normalized the filter to have unit <span class="math inline">\(\ell 2\)</span> norm, i.e. we have already divided all the values in the filter by 3 in the above circumstance (channel = 1). So the final correction factor is <span class="math inline">\(\frac{5}{6}\)</span>, we should divide the <span class="math inline">\(\ell 2\)</span> normalized gradients by this value. Other repeated cells used by general network are recorded below.</p>
<div class="figure">
<img src="http://happynear.wang/images/stride/repeat%20cell.png" alt="Figure 2. Repeated cells of some generally used w and s">
<p class="caption">Figure 2. Repeated cells of some generally used <span class="math inline">\(w\)</span> and <span class="math inline">\(s\)</span></p>
</div>
<p>Another way is to normalize the values in the filters. Since we can modify the filters arbitarily, we may rescale each value in the filter matrix separately. The corners, which are shared by <span class="math inline">\(4\)</span> convolution windows as illustrated in the first figure, need to multiply a factor of <span class="math inline">\(\frac{1}{4}\)</span>. Similarly, we should scale the edge values by <span class="math inline">\(\frac{1}{2}\)</span> and keep the central values unscaled because each of them only locates in one convolution window. The normalize factors of some small kernels are listed below.</p>
<div class="figure">
<img src="http://happynear.wang/images/stride/normalize%20factor.png" alt="Figure 3. Normalize factors of some generally used w and s">
<p class="caption">Figure 3. Normalize factors of some generally used <span class="math inline">\(w\)</span> and <span class="math inline">\(s\)</span></p>
</div>
<p>Analysis of the stride parameter in the average pooling layer is similar. Since it can be seen as convolution layer with flatten filters, the normalization strategy is the same with the first method we discussed above, scaling the whole gradient map by a specified value w.r.t the <span class="math inline">\(w\)</span> and <span class="math inline">\(s\)</span>.</p>
<p>For max-pooling layers, things get different. There is a special case that the max value can be taken from the same position on the feature map but in different window. This circumstance is very common because the image is usually continuous, a maximum value may be the only extreme value in a large region. Look at the two cases below. The max values are taken from different positions in the left case while the two windows share the same max value in the right case. The scale factor will be very different if we still use std as our measurement.</p>
<div class="figure">
<img src="http://happynear.wang/images/stride/max-pooling%20stride.png" alt="Figure 4. Two different cases of the max values’ positions. Cells where the max values are taken are marked by 1.">
<p class="caption">Figure 4. Two different cases of the max values’ positions. Cells where the max values are taken are marked by 1.</p>
</div>
<p>One solution is to use MAD(Mean Abs Deviation) instead of standard deviation as the measurement for the scale. The formulation of MAD is</p>
<p><span class="math display">\[ MAD(x) = \frac{1}{N}\sum_i^N{|x_i-E[x]|}.\]</span></p>
<p>To be compatible with MAD, we need to change our hypothesis introduced in the last two posts from Gaussian distribution to Laplacian distribution. This will be a huge work and I will write the derivation of the formulations in the next post.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/03/28/Normalizing-All-Layers：-Back-Propagation/" itemprop="url">
                  Normalizing All Layers： Back-Propagation
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-03-28T23:50:00+08:00" content="2016-03-28">
              2016-03-28
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/03/28/Normalizing-All-Layers：-Back-Propagation/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/03/28/Normalizing-All-Layers：-Back-Propagation/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/03/28/Normalizing-All-Layers：-Back-Propagation/" class="leancloud_visitors" data-flag-title="Normalizing All Layers： Back-Propagation">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="introduction">1.Introduction</h1>
<p>In the <a href="http://saban.wang/2016/03/22/Normalization-Propagation/" target="_blank" rel="external">last post</a>, we discussed how to make all neurons of a neural network to have normal gaussian distribution. However, as the <a href="http://saban.wang/2016/03/22/Normalization-Propagation/#conclusion" target="_blank" rel="external">Conclusion section</a> claimed, we haven’t considered the back-propagation procedure. In fact, when we talk about the gradient vanishing or exploding problem, we usually refer to the gradients flow in the back-propagation procedure. Since this, the correct way seems to be normalizing the backward gradients of neurons, instead of the forward values.</p>
<p>In this post, we will discuss how to normalize all the gradients using a similar philosophy with the last post: for a given gradient <span class="math inline">\(dy\sim N(0,I)\)</span>, normalizing the layer to make sure that <span class="math inline">\(dx\)</span> is expected to have zero mean and one standard deviation.</p>
<h1 id="parametric-layer">2. Parametric Layer</h1>
<p>Consider the back-propagate fomulation of <code>Convolution</code> and <code>InnerProdcut</code> layer,</p>
<p><span class="math display">\[dx = W dy,\]</span></p>
<p>we will get a similar strategy of normalizing each row of <span class="math inline">\(W\)</span> to be on a <span class="math inline">\(\ell 2\)</span> unit ball. Please note that here we normalize through the fan-out dimension of <span class="math inline">\(W\)</span>, not the fan-in dimension in the forward propagation.</p>
<h1 id="activation-layers">3. Activation Layers</h1>
<p>One problem that can’t be avoided when calculating the formulations of activations is that we should not only assume the distribution of the gradients, but also the forward input of the activation, because the gradients of activations are usually dependent on the inputs. Here we assume that both the input <span class="math inline">\(x\)</span> and the gradient <span class="math inline">\(dy\)</span> follow the normal gaussian distribution <span class="math inline">\(N(0,I)\)</span>, and they are independent with each other.</p>
<h2 id="relu">1) ReLU</h2>
<p>The forward formulation of ReLU is,</p>
<p><span class="math display">\[y = max(0, x).\]</span></p>
<p>Its backward gradients can be easily obtained:</p>
<p><span class="math display">\[dx_i = dy_i * \left\{
\begin{array}{rcl}
1 &amp; &amp; {x_i &gt; 0}\\
0 &amp; &amp; {x_i \leq 0}.
\end{array} \right.\]</span></p>
<p>When <span class="math inline">\(x\sim N(0,I)\)</span>, the gradient of the ReLU layer can be seen as a Bernoulli distribution with probability of 0.5, so the backward mean and standard deviation formulas are similar with those of Dropout layer,</p>
<p><span class="math display">\[E[dx] = 0,\]</span></p>
<p><span class="math display">\[\sigma[dx]=\sqrt{\frac{1}{2}}.\]</span></p>
<p>Here the question comes, now we have two different standard deviations, one for forward values and one for backward gradients, which one should be used to normalize the ReLU layer? My tendency is to use the <span class="math inline">\(\sigma\)</span> calculated by the backward gradients, because backward <span class="math inline">\(\sigma\)</span> is the real murderer of <strong>gradient</strong> vanishing. Moreover, since the bias term is not involved in the backward propagation, it is a good manner to substract the mean <span class="math inline">\(\sqrt{\frac{1}{2\pi}}\)</span> after ReLU activation to ensure zero mean.</p>
<h2 id="sigmoid">2) Sigmoid</h2>
<p>The backward gradient of Sigmoid activation is,</p>
<p><span class="math display">\[ dx = y \cdot (1-y).\]</span></p>
<p>This time, I won’t attempt to calculate the close formulations of mean and std, it is really a tough work. I tend to directly use simulating to get the results.</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="built_in">randn</span>(<span class="number">100000</span>,<span class="number">1</span>);</span><br><span class="line">y = <span class="number">1</span> ./ (<span class="number">1</span> + <span class="built_in">exp</span>(-x));</span><br><span class="line">dy = <span class="built_in">randn</span>(<span class="number">100000</span>,<span class="number">1</span>);</span><br><span class="line">dx = dy .* y .* (<span class="number">1</span>-y);</span><br><span class="line"><span class="built_in">disp</span>([mean(dx) std(dx)]);</span><br></pre></td></tr></table></figure>
<p>We can get <span class="math inline">\(E[dx] = 0\)</span> and <span class="math inline">\(\sigma[dx]=0.2123\)</span>. The same with ReLU, we should still minus the <span class="math inline">\(E[y]=0.5\)</span> after Sigmoid activation and use the <span class="math inline">\(\sigma\)</span> calculated by backward gradients, 0.2123.</p>
<h1 id="pooling-layer">4. Pooling Layer</h1>
<p>The standard deviation of <span class="math inline">\(3\times3\)</span> average pooling can be simulated by,</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dx = [randn(<span class="number">100000</span>,<span class="number">9</span>) / <span class="number">9</span>];</span><br><span class="line"><span class="built_in">disp</span>(std(dx(:)));</span><br></pre></td></tr></table></figure>
<p>It is <span class="math inline">\(\frac{1}{9}\)</span>, and we can infer that the <span class="math inline">\(\sigma\)</span> for <span class="math inline">\(2\times2\)</span> average pooling is <span class="math inline">\(\frac{1}{4}\)</span>.</p>
<p>For max pooling, we only pass the gradient to one of the neurons in the pooling window, so we have,</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dy = <span class="built_in">randn</span>(<span class="number">100000</span>,<span class="number">1</span>);</span><br><span class="line">dx = [dy zeros(<span class="number">100000</span>,<span class="number">8</span>)];</span><br><span class="line"><span class="built_in">disp</span>(std(dx(:)));</span><br></pre></td></tr></table></figure>
<p>Running the script and we can get <span class="math inline">\(\sigma\)</span> for <span class="math inline">\(3\times3\)</span> is <span class="math inline">\(\frac{1}{3}\)</span> and <span class="math inline">\(\sigma\)</span> for <span class="math inline">\(2\times2\)</span> is <span class="math inline">\(\frac{1}{2}\)</span>.</p>
<h1 id="dropout-layer">5. Dropout Layer</h1>
<p>The backward formula for Dropout layer is almost the same with the forward one, we should still divide the preserved values by <span class="math inline">\(\sqrt{q}\)</span> to achieve 1 std for both forward and backward procedure.</p>
<h1 id="conclusion">6. Conclusion</h1>
<p>In this post, we have discussed the normalization strategy that serves the gradient flow of the backward propagation. The mean and std values of forward and backward data flows are listed here:</p>
<table>
<thead>
<tr class="header">
<th align="center">Param</th>
<th align="center">Conv/IP</th>
<th align="center">ReLU</th>
<th align="center">Sigmoid</th>
<th align="center"><span class="math inline">\(3\times3\)</span> Max Pooling</th>
<th align="center">Ave Pooling</th>
<th align="center">Dropout</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">fp mean</td>
<td align="center">0</td>
<td align="center"><span class="math inline">\(\sqrt{\frac{1}{2\pi}}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{2}\)</span></td>
<td align="center">1.4850</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">fp std</td>
<td align="center"><span class="math inline">\(\ell2\)</span> fan-in</td>
<td align="center"><span class="math inline">\(\sqrt{\frac{1}{2} - \frac{1}{2\pi}}\)</span></td>
<td align="center">0.2083</td>
<td align="center">0.5978</td>
<td align="center"><span class="math inline">\(\frac{1}{s}\)</span></td>
<td align="center"><span class="math inline">\(\sqrt{\frac{1}{p}}\)</span></td>
</tr>
<tr class="odd">
<td align="center">bp std</td>
<td align="center"><span class="math inline">\(\ell2\)</span> fan-out</td>
<td align="center"><span class="math inline">\(\sqrt{\frac{1}{2}}\)</span></td>
<td align="center">0.2123</td>
<td align="center"><span class="math inline">\(\frac{1}{3}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{s^2}\)</span></td>
<td align="center"><span class="math inline">\(\sqrt{\frac{1}{p}}\)</span></td>
</tr>
</tbody>
</table>
<p>However, here comes another problem that when we are using the std of backward gradients, the forward value scale would not be controlled well. Inhomogeneous(非齐次) activations, such as sigmoid and tanh, are not suitable for this method because their domain may not cover a sufficient non-linear part of the activation.</p>
<p>So maybe a good choice is to use a separate scaling method for forward and backward propagation? This idea is conflict with the back-propagation algorithm, so we should still carefully examine it through experiment.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/03/22/Normalization-Propagation/" itemprop="url">
                  Normalizing All Layers： A New Standard?
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-03-22T01:45:15+08:00" content="2016-03-22">
              2016-03-22
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/03/22/Normalization-Propagation/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/03/22/Normalization-Propagation/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/03/22/Normalization-Propagation/" class="leancloud_visitors" data-flag-title="Normalizing All Layers： A New Standard?">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>This article is a note on <a href="http://arxiv.org/abs/1603.01431" target="_blank" rel="external">《Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks》</a>.</p>
<h2 id="introduction">1. Introduction</h2>
<p>If you are doing research in Deep Learning, you must know the Batch Normalization[3] techinque, which is a powerful tool to avoid internal covariate shift and gradient vanishing. However, batch normalization only normalize the parametric layers such as convolution layer and innerproduct layer, leaving the chief murderer of gradient vanishing, the activation layers, apart. Another disadvantage of BN is that the it is data-dependent. The network may be unstable when the training samples are in high diversity, or training with small batch size, or our objective is a continuous function such as regression.</p>
<p>In [1], the authors proposed a new standard that if we feed a uniform gaussian distributed data into a network, all the intermediate output should also be uniform gaussian distribute, or at least <strong>expected</strong> to have zero mean and one standard deviation. In this manner, the data flow of the whole network will be very stable, no numerical vanishment or explosion. Since this method is data-independent, it is suitable for regression task and training with batch size of 1.</p>
<h2 id="parametric-layers">2. Parametric Layers</h2>
<p>For parametric layers, such as convolution layer and innerproduct layer, they have a mathematic expression as,</p>
<p><span class="math display">\[y = W^Tx.\]</span></p>
<p>Here we express the convolution layer in a innerproduct way, i.e. using <code>im2col</code> operator to convert the feature map into a wide matrix <span class="math inline">\(x\)</span>.</p>
<p>Now we assume that <span class="math inline">\(x\sim N(0,I)\)</span>, our objective is to let each element in <span class="math inline">\(y\)</span> also follows a uniform gaussian distribution, or at least each value is expected to have zero mean and variance is 1. We can easily find that <span class="math inline">\(E[y]=0\)</span> and</p>
<p><span class="math display">\[Cov[y] = E[yy^T] = E[W^Txx^TW] = W^TE[xx^T]W = W^TW.\]</span></p>
<p>Let <span class="math inline">\(W_i\)</span> to be the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(W\)</span>, then <span class="math inline">\(\Vert W_i\Vert _2\)</span> must equals to 1 to satisfy our target. So a good way to control the variance of each parametric layers’ output is to force each row of the weight matrix to be on a <span class="math inline">\(\ell 2\)</span> unit ball.</p>
<p>To achieve this, we may scale the weight matrix during feed forward,</p>
<p><span class="math display">\[\tilde{W_i} = \frac{W_i}{\Vert W_i \Vert _2},\]</span></p>
<p>and in back propagation a partial derivative is used:</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial W_i} = 
\frac{\frac{\partial \ell}{\partial \tilde{W_i}} - 
\tilde{W_i}\sum_j{\frac{\partial \ell}{\partial \tilde{W_{ij}}}\tilde{W_{ij}}}}
{\Vert W_i \Vert _2}.\]</span></p>
<p>Or, we can directly use the standard back propagation to update <span class="math inline">\(W\)</span> and force to normalize it after each iteration. Which one is better still need examination by experiment.</p>
<h2 id="activation-layers">3. Activation Layers</h2>
<p>Similar with the paramteric layers, we also require the post-activation values to have zero mean and 1 standard deviation.</p>
<h3 id="relu">1) ReLU</h3>
<p>We all know that the formula of ReLU is,</p>
<p><span class="math display">\[y = max(x, 0).\]</span></p>
<p>Assuming <span class="math inline">\(x\sim N(0,I)\)</span>, we can obtain,</p>
<p><span class="math display">\[E[y] = \int_{0}^{+\infty}x\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=
\frac{1}{\sqrt{2\pi}}\int_{0}^{+\infty}e^{-\frac{x^2}{2}}d\frac{x^2}{2}.\]</span></p>
<p>It can be easily got, <span class="math inline">\(E[y] = \sqrt{\frac{1}{2\pi}}\)</span>. Then</p>
<p><span class="math display">\[E[y^2] = \int_{0}^{+\infty}x^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=
\frac{1}{2}\int_{-\infty}^{+\infty}x^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx = \frac{1}{2},\]</span></p>
<p><span class="math display">\[Var[y] = E[y^2] - E[y]^2=\frac{1}{2} - \frac{1}{2\pi}.\]</span></p>
<p>Thus, we should normalize the post-activation of ReLU by substracting <span class="math inline">\(\sqrt{\frac{1}{2\pi}}\)</span> and dividing <span class="math inline">\(\sqrt{\frac{1}{2} - \frac{1}{2\pi}}\)</span>.</p>
<h3 id="sigmoid">2) Sigmoid</h3>
<p>The formula of Sigmoid activation is,</p>
<p><span class="math display">\[y = \frac{1}{1+e^{-x}},\]</span></p>
<p><span class="math display">\[E[y] = \int_{-\infty}^{+\infty}\frac{1}{1+e^{-x}}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx \\
=\int_{-\infty}^{+\infty}(\frac{1}{1+e^{-x}}-\frac{1}{2}+\frac{1}{2})\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx\\
=\int_{-\infty}^{+\infty}(\frac{1}{1+e^{-x}}-\frac{1}{2})\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx +\frac{1}{2}\\
=0+\frac{1}{2}=\frac{1}{2},\]</span></p>
<p><span class="math display">\[E[y^2] = \int_{-\infty}^{+\infty}(\frac{1}{1+e^{-x}})^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx.\]</span></p>
<p>OK, I don’t think we can get a close form of the integral part of <span class="math inline">\(E[y^2]\)</span>. Please note that we are not using the exact form of the equation. What we need is only an empirical value, so we can got the numbers by simulating. With a huge amount of random values, say 100,000, we can get relatively accurate means and standard derivations. By running the script in Matlab,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = randn(100000,1);</span><br><span class="line">y = 1 ./ (1 + exp(-x));</span><br><span class="line">disp([mean(y) std(y)]);</span><br></pre></td></tr></table></figure>
<p>we can get Sigmoid’s standard deviation: <strong>0.2083</strong>. This value can be directly wrote into the program to let the post-sigmoid value have 1 standard deviation.</p>
<h2 id="pooling-layer">4. Pooling Layer</h2>
<p>There are two types of pooling layer, average-pooling and max-pooling. For the average-pooling layer, it is easy to infer that <span class="math inline">\(E[y] = 0\)</span> and <span class="math inline">\(Std[y] = \frac{1}{\sqrt{n}} = \frac{1}{s}\)</span>, where <span class="math inline">\(n\)</span> is the number of neurons in a pooling window or <span class="math inline">\(s\)</span> is the side length of a square pooling window.</p>
<p>For the max-pooling layer, there is no close form expressions too. We still use the simulated values generated by,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = randn(10000000, 9);</span><br><span class="line">y = max(x, [], 2);</span><br><span class="line">disp([mean(y) std(y)]);</span><br></pre></td></tr></table></figure>
<p>The mean value of a <span class="math inline">\(3\times3\)</span> max-pooling is <strong>1.4850</strong> and the standard deviation is <strong>0.5978</strong>. For <span class="math inline">\(2\times2\)</span> max-pooling, mean is <strong>1.0291</strong> and standard deviation is <strong>0.7010</strong>.</p>
<h2 id="dropout-layer">5. Dropout Layer</h2>
<p>Dropout is also a widely used layer in CNN. Although it is claimed to be useless in the NormProp paper, we still would like to record the formulations here. Dropout randomly erase values with a probability of <span class="math inline">\(1-p\)</span>. Now we write it into a mathematic form,</p>
<p><span class="math display">\[y = x \odot r,\]</span></p>
<p>where <span class="math inline">\(r\sim Bernoulli(p)\)</span>. Thus,</p>
<p><span class="math display">\[E[y] = \sum_{i=0,1}\int_{-\infty}^{+\infty}{xr_ip_i\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\\
=0 * \int_{-\infty}^{+\infty}{x(1-p)\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx + 1 * \int_{-\infty}^{+\infty}{xp\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\\
=0\]</span></p>
<p><span class="math display">\[E[y^2] = \sum_{i=0,1}\int_{-\infty}^{+\infty}{(xr_i)^2p_i\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\\
=0+\int_{-\infty}^{+\infty}{x^2p\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\\
=p\]</span></p>
<p><span class="math display">\[Std[y] = \sqrt{E[y^2]-E[y]^2}=\sqrt{p}\]</span></p>
<p>Interestingly, this result is different from what we usually do. We usally preserve values with a ratio of <span class="math inline">\(p\)</span> and divide the preserved values by <span class="math inline">\(p\)</span>, too. Now as we calculated, to achieve 1 s.t.d., we should divide the preserved values by <span class="math inline">\(\sqrt{p}\)</span>. This result should be carefully examined by experiment in the future.</p>
<h2 id="conclusion">6. Conclusion</h2>
<p>In this report, we followed the methodology of [1] to infer the formulation for normalizing all popular layers of a modern CNN. We believe that normalizing every layer with mean substracted and s.t.d. divided will become a standard in the near future. Now we should start to modify our present layers with the new normalization method, and when we are creating new layers, we should keep in mind to normalzie it with the method introduced above.</p>
<p>The shortage of this report is that we haven’t considered the back-propagation procedure. In paper [1][4], they claim that by normalizing the singular values of Jacobian matrix to 1 will lead to faster convergency and more numerically stable. I will study them and explore how to integrate the Jacobian normalization into the present normalization method.</p>
<h2 id="reference">Reference</h2>
<p>[1] Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, Venu Govindaraju, <strong>Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks</strong>. <a href="http://arxiv.org/abs/1603.01431" class="uri" target="_blank" rel="external">http://arxiv.org/abs/1603.01431</a></p>
<p>[2] Tim Salimans, Diederik P. Kingma, Weight Normalization: <strong>A Simple Reparameterization to Accelerate Training of Deep Neural Networks</strong>. <a href="http://arxiv.org/abs/1602.07868" class="uri" target="_blank" rel="external">http://arxiv.org/abs/1602.07868</a></p>
<p>[3] Sergey Ioffe, Christian Szegedy, <strong>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</strong>. <a href="http://arxiv.org/abs/1502.03167" class="uri" target="_blank" rel="external">http://arxiv.org/abs/1502.03167</a></p>
<p>[4] Andrew M. Saxe, James L. McClelland, Surya Ganguli, <strong>Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</strong>. <a href="http://arxiv.org/abs/1312.6120" class="uri" target="_blank" rel="external">http://arxiv.org/abs/1312.6120</a></p>
<h1 id="appendices">Appendices</h1>
<h2 id="a.-the-formula-fault-of-1">A. The formula fault of [1]</h2>
<p>In [1], they present a bound describing the error of using diagnose matrix to approximate a covariance matrix. However, the bound is wrong. They mistake the <span class="math inline">\(\Vert W_i\Vert_2^4\)</span> by <span class="math inline">\(\Vert W_i\Vert_2^2\)</span> in the last line of equation 18. Then the equation 3 and 19 become to:</p>
<p><span class="math display">\[\Vert \Sigma - diag(\alpha) \Vert_F^2\le \sigma^4 \sum_{i,j=1;i\ne j}^m{m(m-1)\mu^2 \Vert W_i \Vert_2^2 \Vert W_j \Vert_2^2}\]</span></p>
<p>As we can see, there is no <span class="math inline">\(1- \Vert W_i \Vert_2^2\)</span> at all, so there is no reason to explain why we should normalize <span class="math inline">\(\Vert W_i \Vert_2^2=1\)</span>.</p>
<p>However, they still get the correct conclusion that we should normalize the weight matrix by the <span class="math inline">\(\ell 2\)</span> norm of rows. But the theoretical analysis of the bound is wrong. In fact, the reason why we should normalize the weight matrix is very simple as we wrote above.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/03/22/Face-Visualization/" itemprop="url">
                  How do you look like in computers’ brain?
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-03-22T01:32:32+08:00" content="2016-03-22">
              2016-03-22
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/03/22/Face-Visualization/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/03/22/Face-Visualization/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/03/22/Face-Visualization/" class="leancloud_visitors" data-flag-title="How do you look like in computers’ brain?">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="introduction">1. Introduction</h1>
<p>Machine Learning researchers may split the learned model into two catogories, generative model and discriminative model, and Convolutional Neural Network is usually seen as a powerful discriminative model. We used to think that the discriminative model have no ability of generating the whole image, just as we human beings, we cannot draw a vivid picture without professional training.</p>
<p>Now we all know that the CNN based system has surpassed human beings in the ability of recognizing people. So do the discriminative CNNs have better ability of drawing portraits, even trained with only discriminative signals? This post will tell you the answer.</p>
<p>Firstly, we need to find a CNN model which is close to the human beings in recognizing people. Thanks <a href="https://github.com/AlfredXiangWu" target="_blank" rel="external">Wu Xiang</a>. He has provided a model with 98.13% verification rate on LFW, while human beings’ average rate is about 99%. We will use <a href="https://github.com/AlfredXiangWu/face_verification_experiment" target="_blank" rel="external">the model</a> he provided for the following experiments.</p>
<h1 id="method">2. Method</h1>
<p>If you are interested in Deep Learning, you must know the <a href="http://googleresearch.blogspot.co.id/2015/06/inceptionism-going-deeper-into-neural.html" target="_blank" rel="external">Inceptionism</a>, which have raised a huge wave of using Neural Networks for art making. Here we will use a similar tool, reversing the neural network. Different from what we usually do, giving an image to a CNN model and it tell us this is Susan or Lucy, now we will tell the neural network from the tail of CNN that we need Susan’s image. Then by back-propagation, we will finally get an image from the input side of the CNN model.</p>
<h1 id="result">3. Result</h1>
<p>Wu Xiang’s model is trained with <a href="http://www.cbsr.ia.ac.cn/english/CASIA-WebFace-Database.html" target="_blank" rel="external">CASIA-WebFace dataset</a>, which is very huge, containing about 490,000 face images of 10,575 movie stars. Now, let’s see choose some movie stars and see what they look like in computer’s brain:</p>
<table>
<thead>
<tr class="header">
<th align="left">Bruce Lee</th>
<th align="center">Mr Bean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/FaceVis/gallery/Bruce%20Lee.png" alt="1"></td>
<td align="center"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/FaceVis/gallery/Mr%20Bean.png" alt="1"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="left">Yun-Fat Chow</th>
<th align="center">Anne Hathaway</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/FaceVis/gallery/Yun-Fat%20Chow.png" alt="1"></td>
<td align="center"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/FaceVis/gallery/Anne%20Hathaway.png" alt="1"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="left">Bingbing Li</th>
<th align="center">Bingbing Fan</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/FaceVis/gallery/Bingbing%20Li.png" alt="1"></td>
<td align="center"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/FaceVis/gallery/Bingbing%20Fan.png" alt="1"></td>
</tr>
</tbody>
</table>
<p>LoL, are these portraits somewhat look like them in the real world? We can see that the CNN have indeedly memorized some features of the training samples, not as the all along prejudice that discriminative model cannot do generating tasks. They can, and do well!</p>
<p>PS: I can’t wait to show the images generated by a pornography detection system. If you have trained one, please contact me!</p>
<p>PS2: The codes are on <a href="https://raw.githubusercontent.com/happynear/DeepVisualization/master/FaceVis" target="_blank" rel="external">my Github</a>. The face verification model can be found in <a href="https://github.com/AlfredXiangWu/face_verification_experiment" target="_blank" rel="external">Wu Xiang’s Github</a>.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/03/21/Visualize-NN-Complexity/" itemprop="url">
                  Visualize the Complexity of Neural Networks
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-03-21T14:06:31+08:00" content="2016-03-21">
              2016-03-21
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/03/21/Visualize-NN-Complexity/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/03/21/Visualize-NN-Complexity/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/03/21/Visualize-NN-Complexity/" class="leancloud_visitors" data-flag-title="Visualize the Complexity of Neural Networks">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>This article is from a failed work. If you can read Mandarine, please see <a href="http://blog.csdn.net/happynear/article/details/46583811" target="_blank" rel="external">this blog</a> for details. I have underestimated the effect of scale &amp; shift in Batch Normalization. <strong>They are very important!</strong></p>
<p>However, I don’t want this work to be thrown into dust basket. I still think that we can get some interesting and direct feelings from the generated images.</p>
<h2 id="brief-algorithm-description">Brief Algorithm Description</h2>
<p>Firstly we take an two channel “slope” image as input.</p>
<table>
<thead>
<tr class="header">
<th align="left">first channel</th>
<th align="center">second channel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/NNComplexity/img/vert.png" alt="1"></td>
<td align="center"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/NNComplexity/img/hori.png" alt="1"></td>
</tr>
</tbody>
</table>
<p>Then we use a randomly initialized (convolutional) nerual network to wrap the slope input to some more complex shapes. Note that a neural network is continuous function w.r.t. the input, the output will also be a continuous but more complex image.</p>
<p>In order to control the range of each layers’ output, we add batch normalization after every convolutional layer as introduced in the original paper. BTW, since we have only one input image, the name “batch normalization” is better to be changed to “spatial normalization”. Without the spatial normalization, the range of the output will get exponential increasement or decreasement with the depth, which is not what we want.</p>
<p>Now we can see how complex the neural network could be. Firstly, with a single layer, 100 hidden channels.</p>
<table>
<thead>
<tr class="header">
<th align="left">ReLU activation</th>
<th align="center">Sigmoid activation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/NNComplexity/img/1conv_relu.png" alt="1"></td>
<td align="center"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/NNComplexity/img/1conv_sigmoid.png" alt="1"></td>
</tr>
</tbody>
</table>
<p>How about 10 layers with 10 hidden channels respectively?</p>
<table>
<thead>
<tr class="header">
<th align="left">ReLU activation</th>
<th align="center">Sigmoid activation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/NNComplexity/img/10conv_relu.png" alt="1"></td>
<td align="center"><img src="https://raw.githubusercontent.com/happynear/DeepVisualization/master/NNComplexity/img/10conv_sigmoid.png" alt="1"></td>
</tr>
</tbody>
</table>
<p>Much more complex, right? Please note that they all have about 100 paramters, but with deeper structure, we produce images with a huge leap in complexity.</p>
<p>We can also apply other sturctures on the input, such as NIN, VGG, Inception etc, and see what’s the difference of them.</p>
<p>The codes are all on <a href="https://github.com/happynear/DeepVisualization/tree/master/NNComplexity" target="_blank" rel="external">my github</a> , you may try them by yourself!</p>
<p>Recently, I noticed that there were similar works long ago. This algorithm is called <a href="https://en.wikipedia.org/wiki/Compositional_pattern-producing_network" target="_blank" rel="external">Compositional pattern-producing network</a> and some other posts also generates beautiful images, such as http://blog.otoro.net/2016/03/25/generating-abstract-patterns-with-tensorflow/ and http://zhouchang.info/blog/2016-02-02/simple-cppn.html .</p>

          
        
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/default_avatar.jpg"
               alt="Feng Wang" />
          <p class="site-author-name" itemprop="name">Feng Wang</p>
          <p class="site-description motion-element" itemprop="description">I am a Ph. D. student at UESTC. My research intrest includes Deep Learning and Facial Image Analysis.</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">5</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/happynear" target="_blank">
                  
                    <i class="fa fa-github"></i> GitHub
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/2548689787/profile" target="_blank">
                  
                    <i class="fa fa-weibo"></i> Weibo
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:wf900911@163.com" target="_blank">
                  
                    <i class="fa fa-globe"></i> Mail
                  
                </a>
              </span>
            
          
        </div>

        
        

        
        <div class="links-of-blogroll motion-element">
          
        </div>

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Feng Wang</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>



      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  


  




<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>

  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=0.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=0.5.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=0.5.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'happynear';
      var disqus_identifier = 'index.html';
      var disqus_title = '';
      var disqus_url = '';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
    </script>
  



  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: false,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>

  
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("5KJQWJkUz5eGYSfYtWp37zNC-gzGzoHsz", "tLGRy72NX6T1cSHprFWQN9a1");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>




</body>
</html>
