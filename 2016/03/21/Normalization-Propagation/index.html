<!doctype html>



  


<html class="theme-next muse use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="true" />










  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.5.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Feng Wang, happynear, deep learning" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=0.5.0" />






<meta name="description" content="This article is a note on 《Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks》. Arxiv link :http://arxiv.org/abs/1603.01431 .
1. Introduction
If">
<meta property="og:type" content="article">
<meta property="og:title" content="Normalize All Layers">
<meta property="og:url" content="http://saban.wang/2016/03/21/Normalization-Propagation/index.html">
<meta property="og:site_name" content="Saban's Blog">
<meta property="og:description" content="This article is a note on 《Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks》. Arxiv link :http://arxiv.org/abs/1603.01431 .
1. Introduction
If">
<meta property="og:updated_time" content="2016-03-21T07:30:18.168Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Normalize All Layers">
<meta name="twitter:description" content="This article is a note on 《Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks》. Arxiv link :http://arxiv.org/abs/1603.01431 .
1. Introduction
If">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: undefined,
      author: 'Author'
    }
  };
</script>

  <title> Normalize All Layers | Saban's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Saban's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags fa-fw"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Normalize All Layers
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2016-03-21T00:44:00+08:00" content="2016-03-21">
              2016-03-21
            </time>
          </span>

          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/03/21/Normalization-Propagation/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/03/21/Normalization-Propagation/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          
             <span id="/2016/03/21/Normalization-Propagation/" class="leancloud_visitors" data-flag-title="Normalize All Layers">
               &nbsp; | &nbsp;
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               <span class="post-meta-item-text">visitors </span>
               <span class="leancloud-visitors-count"></span>
              </span>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This article is a note on 《Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks》. Arxiv link :http://arxiv.org/abs/1603.01431 .</p>
<h2 id="introduction">1. Introduction</h2>
<p>If you are doing research in Deep Learning, you must know the Batch Normalization[3] techinque, which is a powerful tool avoiding internal covariate shift and gradient vanishing. However, batch normalization just normalized the parametric layers such as convolution layer and innerproduct layer. Another disadvantage is that the Batch Normalization is data-dependent. It is unstable during training if the sample are highly diverse or when we are doing regression instead of classification.</p>
<p>In [1], the authors proposed an approach that if we feed a uniform gaussian distributed data into a network, all the intermediate output should also be uniform gaussian distribute. or at least <strong>expected</strong> to have zero mean and one standard deviation. In this manner, the data flow of the whole network will be very stable, no numerical vanishment or explosion. Since this method is data-independent, it is suitable for regression task and training with batch size of 1.</p>
<h2 id="parametric-layers">2. Parametric Layers</h2>
<p>For parametric layers, such as convolution layer and innerproduct layer, they have a mathematic expression as,</p>
<p><span class="math display">\[y = W^Tx.\]</span></p>
<p>Here we express the convolution layer in a innerproduct way, i.e. using <code>im2col</code> operator to convert the feature map into a wide matrix <span class="math inline">\(x\)</span>.</p>
<p>Now we assume that <span class="math inline">\(x\sim N(0,I)\)</span>, our objective is to let each element in <span class="math inline">\(y\)</span> also follows a uniform gaussian distribution, or at least each value is expected to have zero mean and variance is 1. We can easily find that <span class="math inline">\(E[y]=0\)</span> and</p>
<p><span class="math display">\[Var[y] = E[yy^T] = E[W^Txx^TW] = W^TE[xx^T]W = W^TW.\]</span></p>
<p>Let <span class="math inline">\(W_i\)</span> to be the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(W\)</span>, then <span class="math inline">\(\Vert W_i\Vert _2\)</span> must equals to 1 to satisfy our target. So a good way to control the variance of each parametric layers’ output is to force each row of the weight matrix to be on a <span class="math inline">\(\ell 2\)</span> unit ball.</p>
<p>To achieve this, we may scale the weight matrix during feed forward,</p>
<p><span class="math display">\[\tilde{W_i} = \frac{W_i}{\Vert W_i \Vert _2},\]</span></p>
<p>and in back propagation a partial derivative is used:</p>
<p><span class="math display">\[\frac{\partial \ell}{\partial W_i} = 
\frac{\frac{\partial \ell}{\partial \tilde{W_i}} - 
\tilde{W_i}\sum_j{\frac{\partial \ell}{\partial \tilde{W_{ij}}}\tilde{W_{ij}}}}
{\Vert W_i \Vert _2}.\]</span></p>
<p>Or, we can directly use the standard back propagation to update <span class="math inline">\(W\)</span> and force to normalize it after each iteration. Which one is better still need examination by experiment.</p>
<h2 id="activation-layers">3. Activation Layers</h2>
<p>Similar with the paramteric layers, we also require the post-activation values to have zero mean and 1 standard deviation.</p>
<h3 id="relu">1) ReLU</h3>
<p>We all know that the formula of ReLU is,</p>
<p><span class="math display">\[y = max(x, 0).\]</span></p>
<p>Assuming <span class="math inline">\(x\sim N(0,I)\)</span>, we can obtain,</p>
<p><span class="math display">\[E[y] = \int_{0}^{+\infty}x\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=
\frac{1}{\sqrt{2\pi}}\int_{0}^{+\infty}e^{-\frac{x^2}{2}}d\frac{x^2}{2}.\]</span></p>
<p>It can be easily got, <span class="math inline">\(E[y] = \sqrt{\frac{1}{2\pi}}\)</span>. Then</p>
<p><span class="math display">\[E[y^2] = \int_{0}^{+\infty}x^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=
\frac{1}{2}\int_{-\infty}^{+\infty}x^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx = \frac{1}{2},\]</span></p>
<p><span class="math display">\[Var[y] = E[y^2] - E[y]^2=\frac{1}{2} - \frac{1}{2\pi}.\]</span></p>
<p>Thus, we should normalize the post-activation of ReLU by substracting <span class="math inline">\(\sqrt{\frac{1}{2\pi}}\)</span> and dividing <span class="math inline">\(\sqrt{\frac{1}{2} - \frac{1}{2\pi}}\)</span>.</p>
<h3 id="sigmoid">2) Sigmoid</h3>
<p>The formula of Sigmoid activation is,</p>
<p><span class="math display">\[y = \frac{1}{1+e^{-x}},\]</span></p>
<p><span class="math display">\[E[y] = \int_{-\infty}^{+\infty}\frac{1}{1+e^{-x}}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx \\
=\int_{-\infty}^{+\infty}(\frac{1}{1+e^{-x}}-\frac{1}{2}+\frac{1}{2})\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx\\
=\int_{-\infty}^{+\infty}(\frac{1}{1+e^{-x}}-\frac{1}{2})\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx +\frac{1}{2}\\
=0+\frac{1}{2}=\frac{1}{2},\]</span></p>
<p><span class="math display">\[E[y^2] = \int_{-\infty}^{+\infty}(\frac{1}{1+e^{-x}})^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx.\]</span></p>
<p>OK, I don’t think we can get a close form of the integral part <span class="math inline">\(E[y^2]\)</span>. Please note that we are not using the exact form of the equation. What we need is only an empirical value, so we can got the numbers by simulating. With a huge amount of random values, say 100,000, we can get relatively accurate means and standard derivations. By running the script in Matlab,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = randn(100000,1);</span><br><span class="line">y = 1 ./ (1 + exp(-x));</span><br><span class="line">disp([mean(y) std(y)]);</span><br></pre></td></tr></table></figure>
<p>We can get Sigmoid’s standard deviation: <strong>0.2083</strong>. This value can be directly wrote into the program to let the post-sigmoid value have 1 standard deviation.</p>
<h2 id="pooling-layer">4. Pooling Layer</h2>
<p>There are two types of pooling layer, average-pooling and max-pooling. For the average-pooling layer, it is easy to infer that <span class="math inline">\(E[y] = 0\)</span> and <span class="math inline">\(Std[y] = \frac{1}{\sqrt{n}} = \frac{1}{s}\)</span>, where <span class="math inline">\(n\)</span> is the number of neurons in a pooling window or <span class="math inline">\(s\)</span> is the side length of a square pooling window.</p>
<p>For the max-pooling layer, there is no close form expressions too. We should still using simulated values generated by,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = randn(10000000, 9);</span><br><span class="line">y = max(x, [], 2);</span><br><span class="line">disp([mean(y) std(y)]);</span><br></pre></td></tr></table></figure>
<p>The mean value of a <span class="math inline">\(3\times3\)</span> max-pooling is <strong>1.4850</strong> and the standard deviation is <strong>0.5978</strong>. For <span class="math inline">\(2\times2\)</span> max-pooling, mean is <strong>1.0291</strong> and standard deviation is <strong>0.7010</strong>.</p>
<h2 id="dropout-layer">5. Dropout Layer</h2>
<p>Dropout is also a widely used layer in CNN. Although it is claimed to be useless in the NormProp paper, we still would like to record the formulations here. Dropout randomly erase values with a probability of <span class="math inline">\(1-p\)</span>. Now we write it into a mathematic form,</p>
<p><span class="math display">\[y = x \odot r,\]</span></p>
<p>where <span class="math inline">\(r\sim Bernoulli(p)\)</span>. Thus,</p>
<p><span class="math display">\[E[y] = \sum_{i=0,1}\int_{-\infty}^{+\infty}{xr_ip_i\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\\
=0 * \int_{-\infty}^{+\infty}{x(1-p)\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx + 1 * \int_{-\infty}^{+\infty}{xp\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\\
=0\]</span></p>
<p><span class="math display">\[E[y^2] = \sum_{i=0,1}\int_{-\infty}^{+\infty}{(xr_i)^2p_i\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\\
=0+\int_{-\infty}^{+\infty}{x^2p\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\\
=p\]</span></p>
<p><span class="math display">\[Std[y] = \sqrt{E[y^2]-E[y]^2}=\sqrt{p}\]</span></p>
<p>Interestingly, this result is different from what we usually do. We usally preserve values with a ratio of <span class="math inline">\(p\)</span> and divide the preserved values by <span class="math inline">\(p\)</span>, too. Now as we calculated, to achieve 1 s.t.d., we should divide the preserved values by <span class="math inline">\(\sqrt{p}\)</span>. This inference should be carefully examined by experiment in the future.</p>
<h2 id="conclusion">6. Conclusion</h2>
<p>In this report, we followed the methodology of [1] to infer the formulation for normalizing all popular layers of a modern CNN. We believe that normalizing every layer with mean substracted and standard deviation divided will become a standard in the near future. When we are creating new layers, we should do one more step of calculating the simulated mean and standard deviation then normalize the output of the new layers.</p>
<p>The shortage of this report is that we haven’t considered the back-propagation procedure. In paper [1][4], they claim that by normalizing the s`ingular values of Jacobian matrix to 1 will lead to faster convergency and more numerically stable. I will study them and explore how to integrate the Jacobian normalization into the present normalization method.</p>
<h2 id="reference">Reference</h2>
<p>[1] Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, Venu Govindaraju, Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks. http://arxiv.org/abs/1603.01431</p>
<p>[2] Tim Salimans, Diederik P. Kingma, Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. http://arxiv.org/abs/1602.07868</p>
<p>[3] Sergey Ioffe, Christian Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. http://arxiv.org/abs/1502.03167</p>
<p>[4] Andrew M. Saxe, James L. McClelland, Surya Ganguli, Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. http://arxiv.org/abs/1312.6120</p>
<h1 id="appendices">Appendices</h1>
<h2 id="a.-the-formula-fault-of-1">A. The formula fault of [1]</h2>
<p>In [1], they present a bound describing the error of using diagnose matrix to approximate a covariance matrix. However, the bound is wrong. They mistake the <span class="math inline">\(\Vert W_i\Vert_2^4\)</span> by <span class="math inline">\(\Vert W_i\Vert_2^2\)</span> in the last line of equation 18.</p>
<p>However, they still get the same conclusion that we should normalize the weight matrix by the <span class="math inline">\(\ell 2\)</span> norm of rows. But the theoretical analysis of the bound is wrong. In fact, the reason why we should normalize the weight matrix is very simple as we reported above.</p>

      
    </div>

    <div>
      
        
      
    </div>

    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/03/21/Visualize-NN-Complexity/" rel="prev" title="Visualize the Complexity of Neural Networks">
                Visualize the Complexity of Neural Networks <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/default_avatar.jpg"
               alt="Feng Wang" />
          <p class="site-author-name" itemprop="name">Feng Wang</p>
          <p class="site-description motion-element" itemprop="description">I am a Ph. D. student at UESTC. My research intrest includes Deep Learning and Facial Image Analysis.</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">2</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/happynear" target="_blank">
                  
                    <i class="fa fa-github"></i> GitHub
                  
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/2548689787/profile" target="_blank">
                  
                    <i class="fa fa-weibo"></i> Weibo
                  
                </a>
              </span>
            
          
        </div>

        
        

        
        <div class="links-of-blogroll motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator">
            <i class="fa fa-angle-double-up"></i>
          </div>
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#parametric-layers"><span class="nav-number">2.</span> <span class="nav-text">2. Parametric Layers</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#activation-layers"><span class="nav-number">3.</span> <span class="nav-text">3. Activation Layers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#relu"><span class="nav-number">3.1.</span> <span class="nav-text">1) ReLU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sigmoid"><span class="nav-number">3.2.</span> <span class="nav-text">2) Sigmoid</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pooling-layer"><span class="nav-number">4.</span> <span class="nav-text">4. Pooling Layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dropout-layer"><span class="nav-number">5.</span> <span class="nav-text">5. Dropout Layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#conclusion"><span class="nav-number">6.</span> <span class="nav-text">6. Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reference"><span class="nav-number">7.</span> <span class="nav-text">Reference</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#appendices"><span class="nav-number"></span> <span class="nav-text">Appendices</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#a.-the-formula-fault-of-1"><span class="nav-number">1.</span> <span class="nav-text">A. The formula fault of [1]</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator">
            <i class="fa fa-angle-double-down"></i>
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Feng Wang</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>



      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  


  




<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>

  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=0.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=0.5.0"></script>



  
  

  
  
<script type="text/javascript" src="/js/src/scrollspy.js?v=0.5.0"></script>

<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 1 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = NexT.utils.escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    NexT.motion.middleWares.sidebar = function () {
      var $tocContent = $('.post-toc-content');

      if (CONFIG.sidebar.display === 'post' || CONFIG.sidebar.display === 'always') {
        if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
          NexT.utils.displaySidebar();
        }
      }
    };
  });
</script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=0.5.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'happynear';
      var disqus_identifier = '2016/03/21/Normalization-Propagation/';
      var disqus_title = 'Normalize All Layers';
      var disqus_url = 'http://saban.wang/2016/03/21/Normalization-Propagation/';

      function run_disqus_script(disqus_script){
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');
      
        run_disqus_script('embed.js');
      
    </script>
  



  
  
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: false,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>

  
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("5KJQWJkUz5eGYSfYtWp37zNC-gzGzoHsz", "p6Je7gJMDPs9VUYO8t3IX3yI");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>




</body>
</html>
