<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Normalize All Layers | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This article is a note on 《Normalization Propagation: A Parametric Technique for Removing InternalCovariate Shift in Deep Networks》. Arxiv link :http://arxiv.org/abs/1603.01431 .
1. IntroductionIf you">
<meta property="og:type" content="article">
<meta property="og:title" content="Normalize All Layers">
<meta property="og:url" content="http://saban.wang/2016/03/21/Normalization-Propagation/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="This article is a note on 《Normalization Propagation: A Parametric Technique for Removing InternalCovariate Shift in Deep Networks》. Arxiv link :http://arxiv.org/abs/1603.01431 .
1. IntroductionIf you">
<meta property="og:updated_time" content="2016-03-20T18:24:04.278Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Normalize All Layers">
<meta name="twitter:description" content="This article is a note on 《Normalization Propagation: A Parametric Technique for Removing InternalCovariate Shift in Deep Networks》. Arxiv link :http://arxiv.org/abs/1603.01431 .
1. IntroductionIf you">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://saban.wang"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Normalization-Propagation" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/03/21/Normalization-Propagation/" class="article-date">
  <time datetime="2016-03-20T16:44:00.000Z" itemprop="datePublished">2016-03-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Normalize All Layers
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>This article is a note on 《Normalization Propagation: A Parametric Technique for Removing Internal<br>Covariate Shift in Deep Networks》. Arxiv link :<a href="http://arxiv.org/abs/1603.01431" target="_blank" rel="external">http://arxiv.org/abs/1603.01431</a> .</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>If you are doing research in Deep Learning, you must know the Batch Normalization[3] techinque, which is a powerful tool avoiding internal covariate shift and gradient vanishing.  However, batch normalization just normalized the parametric layers such as convolution layer and innerproduct layer.  Another disadvantage is that the Batch Normalization is data-dependent. It is unstable during training if the sample are highly diverse or when we are doing regression instead of classification.</p>
<p>In [1], the authors proposed an approach that if we feed a uniform gaussian distributed data into a network, all the intermediate output should also be uniform gaussian distribute. or at least <strong>expected</strong> to have zero mean and one standard deviation. In this manner, the data flow of the whole network will be very stable, no numerical vanishment or explosion. Since this method is data-independent, it is suitable for regression task and training with batch size of 1.</p>
<h2 id="2-Parametric-Layers"><a href="#2-Parametric-Layers" class="headerlink" title="2. Parametric Layers"></a>2. Parametric Layers</h2><p>For parametric layers, such as convolution layer and innerproduct layer, they have a mathematic expression as,</p>
<p>$$y = W^Tx.$$</p>
<p>Here we express the convolution layer in a innerproduct way, i.e. using <code>im2col</code> operator to convert the feature map into a wide matrix $x$. </p>
<p>Now we assume that $x\sim N(0,I)$, our objective is to let each element in $y$ also follows a uniform gaussian distribution, or at least each value is expected to have zero mean and variance is 1. We can easily find that $E[y]=0$ and </p>
<p>$$Var[y] = E[yy^T] = E[W^Txx^TW] = W^TE[xx^T]W = W^TW.$$</p>
<p>Let $W_i$ to be the $i$-th row of $W$, then $\Vert W_i\Vert _2$ must equals to 1 to satisfy our target. So a good way to control the variance of each parametric layers’ output is to force each row of the weight matrix to be on a $\ell 2$ unit ball.</p>
<p>To achieve this, we may scale the weight matrix during feed forward,</p>
<p>$$\tilde{W_i} = \frac{W_i}{\Vert W_i \Vert _2},$$</p>
<p>and in back propagation a partial derivative is used:</p>
<p>$$\frac{\partial \ell}{\partial W_i} =<br>\frac{\frac{\partial \ell}{\partial \tilde{W_i}} -<br>\tilde{W_i}\sum<em>j{\frac{\partial \ell}{\partial \tilde{W</em>{ij}}}\tilde{W_{ij}}}}<br>{\Vert W_i \Vert _2}.$$</p>
<p>Or, we can directly use the standard back propagation to update $W$ and force to normalize it after each iteration. Which one is better still need examination by experiment.</p>
<h2 id="3-Activation-Layers"><a href="#3-Activation-Layers" class="headerlink" title="3. Activation Layers"></a>3. Activation Layers</h2><p>Similar with the paramteric layers, we also require the post-activation values to have zero mean and 1 standard deviation.</p>
<h3 id="1-ReLU"><a href="#1-ReLU" class="headerlink" title="1) ReLU"></a>1) ReLU</h3><p>We all know that the formula of ReLU is,</p>
<p>$$y = max(x, 0).$$</p>
<p>Assuming $x\sim N(0,I)$, we can obtain,</p>
<p>$$E[y] = \int<em>{0}^{+\infty}x\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=<br>\frac{1}{\sqrt{2\pi}}\int</em>{0}^{+\infty}e^{-\frac{x^2}{2}}d\frac{x^2}{2}.$$</p>
<p>It can be easily got, $E[y] = \sqrt{\frac{1}{2\pi}}$. Then </p>
<p>$$E[y^2] = \int<em>{0}^{+\infty}x^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx=<br>\frac{1}{2}\int</em>{-\infty}^{+\infty}x^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx = \frac{1}{2},$$</p>
<p>$$Var[y] = E[y^2] - E[y]^2=\frac{1}{2} - \frac{1}{2\pi}.$$</p>
<p>Thus, we should normalize the post-activation of ReLU by substracting $\sqrt{\frac{1}{2\pi}}$ and dividing $\sqrt{\frac{1}{2} - \frac{1}{2\pi}}$.</p>
<h3 id="2-Sigmoid"><a href="#2-Sigmoid" class="headerlink" title="2) Sigmoid"></a>2) Sigmoid</h3><p>The formula of Sigmoid activation is,</p>
<p>$$y = \frac{1}{1+e^{-x}},$$</p>
<p>$$E[y] = \int<em>{-\infty}^{+\infty}\frac{1}{1+e^{-x}}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx \<br>=\int</em>{-\infty}^{+\infty}(\frac{1}{1+e^{-x}}-\frac{1}{2}+\frac{1}{2})\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx\<br>=\int_{-\infty}^{+\infty}(\frac{1}{1+e^{-x}}-\frac{1}{2})\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx +\frac{1}{2}\<br>=0+\frac{1}{2}=\frac{1}{2},$$</p>
<p>$$E[y^2] = \int_{-\infty}^{+\infty}(\frac{1}{1+e^{-x}})^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx.$$</p>
<p>OK, I don’t think we can get a close form of the integral part $E[y^2]$. Please note that we are not using the exact form of the equation. What we need is only an empirical value, so we can got the numbers by simulating. With a huge amount of random values, say 100,000, we can get relatively accurate  means and standard derivations. By running the script in Matlab,</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = randn(<span class="number">100000,1</span>)<span class="comment">;</span></span><br><span class="line">y = <span class="number">1</span> ./ (<span class="number">1</span> + exp(-x))<span class="comment">;</span></span><br><span class="line">disp([mean(y) std(y)])<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>We can get Sigmoid’s standard deviation: <strong>0.2083</strong>. This value can be directly wrote into the program to let the post-sigmoid value have 1 standard deviation.</p>
<h2 id="4-Pooling-Layer"><a href="#4-Pooling-Layer" class="headerlink" title="4. Pooling Layer"></a>4. Pooling Layer</h2><p>There are two types of pooling layer, average-pooling and max-pooling. For the average-pooling layer, it is easy to infer that $E[y] = 0$ and $Std[y] = \frac{1}{\sqrt{n}} = \frac{1}{s}$, where $n$ is the number of neurons in a pooling window or $s$ is the side length of a square pooling window. </p>
<p>For the max-pooling layer, there is no close form expressions too. We should still using simulated values generated by,</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = randn(<span class="number">10000000</span>, <span class="number">9</span>)<span class="comment">;</span></span><br><span class="line">y = max(<span class="name">x</span>, [], <span class="number">2</span>)<span class="comment">;</span></span><br><span class="line">disp([mean(<span class="name">y</span>) std(<span class="name">y</span>)])<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>The mean value of a $3\times3$ max-pooling is <strong>1.4850</strong> and the standard deviation is <strong>0.5978</strong>. For $2\times2$ max-pooling, mean is <strong>1.0291</strong> and standard deviation is <strong>0.7010</strong>.</p>
<h2 id="5-Dropout-Layer"><a href="#5-Dropout-Layer" class="headerlink" title="5. Dropout Layer"></a>5. Dropout Layer</h2><p>Dropout is also a widely used layer in CNN. Although it is claimed to be useless in the NormProp paper, we still would like to record the formulations here. Dropout randomly erase values with a probability of $1-p$. Now we write it into a mathematic form,</p>
<p>$$y = x \odot r,$$</p>
<p>where $r\sim Bernoulli(p)$. Thus,</p>
<p>$$E[y] = \sum<em>{i=0,1}\int</em>{-\infty}^{+\infty}{xr_ip<em>i\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\<br>=0 * \int</em>{-\infty}^{+\infty}{x(1-p)\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx + 1 * \int_{-\infty}^{+\infty}{xp\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\<br>=0$$</p>
<p>$$E[y^2] = \sum<em>{i=0,1}\int</em>{-\infty}^{+\infty}{(xr_i)^2p<em>i\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\<br>=0+\int</em>{-\infty}^{+\infty}{x^2p\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}}dx\<br>=p$$</p>
<p>$$Std[y] = \sqrt{E[y^2]-E[y]^2}=\sqrt{p}$$</p>
<p>Interestingly, this result is different from what we usually do. We usally preserve values with a ratio of $p$ and divide the preserved values by $p$, too. Now as we calculated, to achieve 1 s.t.d., we should divide the preserved values by $\sqrt{p}$. This inference should be carefully examined by experiment in the future.</p>
<h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>In this report, we followed the methodology of [1] to infer the formulation for normalizing all popular layers of a modern CNN. We believe that normalizing every layer with mean substracted and standard deviation divided will become a standard in the near future. When we are creating new layers, we should do one more step of calculating the simulated mean and standard deviation then normalize the output of the new layers.</p>
<p>The shortage of this report is that we haven’t considered the back-propagation procedure. In paper [1][4], they claim that by normalizing the s`ingular values of Jacobian matrix to 1 will lead to faster convergency and more numerically stable. I will study them and explore how to integrate the Jacobian normalization into the present normalization method.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, Venu Govindaraju, Normalization Propagation: A Parametric Technique for Removing Internal<br>Covariate Shift in Deep Networks. <a href="http://arxiv.org/abs/1603.01431" target="_blank" rel="external">http://arxiv.org/abs/1603.01431</a><br>[2] Tim Salimans, Diederik P. Kingma, Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. <a href="http://arxiv.org/abs/1602.07868" target="_blank" rel="external">http://arxiv.org/abs/1602.07868</a><br>[3] Sergey Ioffe, Christian Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. <a href="http://arxiv.org/abs/1502.03167" target="_blank" rel="external">http://arxiv.org/abs/1502.03167</a><br>[4] Andrew M. Saxe, James L. McClelland, Surya Ganguli, Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. <a href="http://arxiv.org/abs/1312.6120" target="_blank" rel="external">http://arxiv.org/abs/1312.6120</a></p>
<h1 id="Appendices"><a href="#Appendices" class="headerlink" title="Appendices"></a>Appendices</h1><h2 id="A-The-formula-fault-of-1"><a href="#A-The-formula-fault-of-1" class="headerlink" title="A. The formula fault of [1]"></a>A. The formula fault of [1]</h2><p>In [1], they present a bound describing the error of using diagnose matrix to approximate a covariance matrix. However, the bound is wrong. They mistake the $\Vert W_i\Vert_2^4$ by $\Vert W_i\Vert_2^2$ in the last line of equation 18.</p>
<p>However, they still get the same conclusion that we should normalize the weight matrix by the $\ell 2$ norm of rows. But the theoretical analysis of the bound is wrong. In fact, the reason why we should normalize the weight matrix is very simple as we reported above.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://saban.wang/2016/03/21/Normalization-Propagation/" data-id="cim0wk90t0000zoxkjny1s12t" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/03/21/hello-world/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Hello World
        
      </div>
    </a>
  
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  
    
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/03/21/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2016/03/21/Normalization-Propagation/">Normalize All Layers</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 Feng Wang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>